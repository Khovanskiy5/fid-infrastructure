x-service-defaults: &service_defaults
  restart: unless-stopped
  networks:
    - infrastructure

services:
  # Прокси с TLS
  nginx:
    image: nginx:1.23.1-alpine
    container_name: nginx
    hostname: nginx
    <<: *service_defaults
    depends_on:
      php-fpm:
        condition: service_started
    ports:
      - "${NGINX_HTTP_PORT:-80}:80"
      - "${NGINX_HTTPS_PORT:-443}:443"
    volumes:
      - ./services/nginx/conf/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./services/nginx/conf/conf.d:/etc/nginx/conf.d:ro
      - ./services/nginx/certs:/etc/nginx/certs:ro
      - ./logs/nginx:/var/log/nginx
      - ./src:/var/www/html:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1/healthz >/dev/null || curl -fsS http://127.0.0.1/healthz >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s

  # Веб приложение
  php-fpm:
    build:
      context: ./services/php/fpm
      dockerfile: Dockerfile
    image: local/php-fpm:alpine
    container_name: php-fpm
    hostname: php-fpm
    <<: *service_defaults
    environment:
      - PHP_MEMORY_LIMIT=512M
    volumes:
      - ./src:/var/www/html
      - ./logs/php:/var/log/php
      - ./services/php/fpm/conf/php.ini:/usr/local/etc/php/php.ini:ro
      - ./services/php/fpm/conf/www.conf:/usr/local/etc/php-fpm.d/www.conf:ro

  nodejs:
    build:
      context: ./services/nodejs
      dockerfile: Dockerfile
    image: local/node:alpine
    container_name: nodejs
    hostname: nodejs
    <<: *service_defaults
    command: ["node", "-e", "const http=require('http');const port=parseInt(process.env.HEALTH_PORT||'3000',10);http.createServer((req,res)=>{if(req.url==='/health'){res.writeHead(200,{'Content-Type':'text/plain'});return res.end('OK');}res.statusCode=404;res.end('Not Found');}).listen(port,()=>console.log('Health server on',port));process.on('SIGTERM',()=>process.exit(0));"]
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:$${HEALTH_PORT:-3000}/health | grep -q 'OK'"]
      interval: 5s
      timeout: 2s
      retries: 5
      start_period: 3s
    volumes:
      - ./src:/usr/src/app
      - ./logs/nodejs:/var/log/node

  # PostgreSQL кластер + PGBouncer + Haproxy
  postgres-master:
    image: bitnami/postgresql-repmgr:15.10.0
    container_name: postgres-master
    hostname: postgres-master
    <<: *service_defaults
    environment:
      - REPMGR_PASSWORD=${POSTGRES_REPMGR_PASSWORD}
      - POSTGRESQL_POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRESQL_USERNAME=${POSTGRES_USER}
      - POSTGRESQL_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRESQL_DATABASE=${POSTGRES_DB}
      - REPMGR_PRIMARY_ROLE_WAIT_FOR_SYNC=on
      - REPMGR_NODE_ID=1
      - REPMGR_NODE_NAME=pg-0
      - REPMGR_NODE_NETWORK_NAME=postgres-master
      - REPMGR_PRIMARY_HOST=postgres-master
      - REPMGR_PARTNER_NODES=postgres-master,postgres-replica
      - REPMGR_PORT_NUMBER=5432
      - POSTGRESQL_SHARED_PRELOAD_LIBRARIES=repmgr
      - POSTGRESQL_LOG_HOSTNAME=true
      - POSTGRESQL_LOG_CONNECTIONS=true
    volumes:
      - postgres-master-data:/bitnami/postgresql
      - ./logs/postgres/master:/bitnami/postgresql/logs
    ports:
      - "${POSTGRES_MASTER_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h 127.0.0.1 -p 5432 -U $${POSTGRESQL_USERNAME:-postgres} || /opt/bitnami/postgresql/bin/pg_isready -h 127.0.0.1 -p 5432 -U $${POSTGRESQL_USERNAME:-postgres}"]
      interval: 5s
      timeout: 3s
      retries: 6
      start_period: 30s

  postgres-replica:
    image: bitnami/postgresql-repmgr:15.10.0
    container_name: postgres-replica
    hostname: postgres-replica
    <<: *service_defaults
    depends_on:
      - postgres-master
    environment:
      - REPMGR_PASSWORD=${POSTGRES_REPMGR_PASSWORD}
      - POSTGRESQL_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRESQL_USERNAME=${POSTGRES_USER}
      - REPMGR_NODE_ID=2
      - REPMGR_NODE_NAME=pg-1
      - REPMGR_NODE_NETWORK_NAME=postgres-replica
      - REPMGR_PRIMARY_HOST=postgres-master
      - REPMGR_PARTNER_NODES=postgres-master,postgres-replica
      - REPMGR_PORT_NUMBER=5432
      - REPMGR_PRIMARY_ROLE_WAIT_FOR_SYNC=on
    volumes:
      - postgres-replica-data:/bitnami/postgresql
      - ./logs/postgres/replica:/bitnami/postgresql/logs
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h 127.0.0.1 -p 5432 -U $${POSTGRESQL_USERNAME:-postgres} || /opt/bitnami/postgresql/bin/pg_isready -h 127.0.0.1 -p 5432 -U $${POSTGRESQL_USERNAME:-postgres}"]
      interval: 5s
      timeout: 3s
      retries: 8
      start_period: 40s

  pgbouncer:
    image: bitnami/pgbouncer:1.24.0
    container_name: pgbouncer
    hostname: pgbouncer
    <<: *service_defaults
    environment:
      - POSTGRESQL_HOST=postgres-master
      - POSTGRESQL_PORT_NUMBER=5432
      - POSTGRESQL_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRESQL_USERNAME=${POSTGRES_USER}
      - PGBOUNCER_POOL_MODE=session
      - PGBOUNCER_MAX_CLIENT_CONN=500
      - PGBOUNCER_DEFAULT_POOL_SIZE=50
      - PGBOUNCER_LISTEN_ADDR=0.0.0.0
      - PGBOUNCER_LISTEN_PORT=6432
    volumes:
      - ./services/pgbouncer/pgbouncer.ini:/bitnami/pgbouncer/conf/pgbouncer.ini:ro
      - ./logs/pgbouncer:/bitnami/pgbouncer/logs
    depends_on:
      - postgres-master
    healthcheck:
      test: ["CMD-SHELL", "pgrep -x pgbouncer >/dev/null 2>&1 || ps | grep -q '[p]gbouncer'"]
      interval: 5s
      timeout: 2s
      retries: 3
      start_period: 5s


  # Redis master/slave + Sentinel кластер
  redis-master:
    container_name: redis-master
    hostname: redis-master
    <<: *service_defaults
    image: bitnami/redis:6.2.6
    environment:
      - REDIS_REPLICATION_MODE=master
      - ALLOW_EMPTY_PASSWORD=yes
    ports:
      - "${REDIS_MASTER_PORT:-6379}:6379"
    volumes:
      - redis-master-data:/bitnami/redis/data
      - ./logs/redis/master:/opt/bitnami/redis/logs
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "127.0.0.1", "-p", "6379", "ping"]
      interval: 5s
      timeout: 2s
      retries: 3
      start_period: 5s

  redis-slave:
    container_name: redis-slave
    hostname: redis-slave
    <<: *service_defaults
    image: bitnami/redis:6.2.6
    environment:
      - REDIS_REPLICATION_MODE=slave
      - REDIS_MASTER_HOST=redis-master
      - ALLOW_EMPTY_PASSWORD=yes
    ports:
      - '6379'
    depends_on:
      redis-master:
        condition: service_healthy
    volumes:
      - redis-slave-data:/bitnami/redis/data
      - ./logs/redis/slave:/bitnami/log/redis
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "127.0.0.1", "-p", "6379", "ping"]
      interval: 5s
      timeout: 2s
      retries: 5
      start_period: 12s

  sentinel-1:
    container_name: sentinel-1
    hostname: sentinel-1
    <<: *service_defaults
    image: bitnami/redis-sentinel:6.2.6
    depends_on:
      redis-master:
        condition: service_healthy
      redis-slave:
        condition: service_healthy
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
      - REDIS_MASTER_HOST=redis-master
      - REDIS_MASTER_PORT_NUMBER=6379
      - REDIS_SENTINEL_RESOLVE_HOSTNAMES=yes
      - REDIS_SENTINEL_ANNOUNCE_HOSTNAMES=yes
    ports:
      - "${REDIS_SENTINEL1_PORT:-26379}:26379"
    volumes:
      - ./logs/redis/sentinel1:/opt/bitnami/redis-sentinel/logs
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "127.0.0.1", "-p", "26379", "ping"]
      interval: 5s
      timeout: 2s
      retries: 5
      start_period: 5s

  sentinel-2:
    container_name: sentinel-2
    hostname: sentinel-2
    <<: *service_defaults
    image: bitnami/redis-sentinel:6.2.6
    depends_on:
      - redis-master
      - redis-slave
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
      - REDIS_MASTER_HOST=redis-master
      - REDIS_MASTER_PORT_NUMBER=6379
      - REDIS_SENTINEL_RESOLVE_HOSTNAMES=yes
      - REDIS_SENTINEL_ANNOUNCE_HOSTNAMES=yes
    ports:
      - "${REDIS_SENTINEL2_PORT:-26380}:26379"
    volumes:
      - ./logs/redis/sentinel2:/opt/bitnami/redis-sentinel/logs
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "127.0.0.1", "-p", "26379", "ping"]
      interval: 5s
      timeout: 2s
      retries: 5
      start_period: 5s

  sentinel-3:
    container_name: sentinel-3
    hostname: sentinel-3
    <<: *service_defaults
    image: bitnami/redis-sentinel:6.2.6
    depends_on:
      redis-master:
        condition: service_healthy
      redis-slave:
        condition: service_healthy
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
      - REDIS_MASTER_HOST=redis-master
      - REDIS_MASTER_PORT_NUMBER=6379
      - REDIS_SENTINEL_RESOLVE_HOSTNAMES=yes
      - REDIS_SENTINEL_ANNOUNCE_HOSTNAMES=yes
    ports:
      - "${REDIS_SENTINEL3_PORT:-26381}:26379"
    volumes:
      - ./logs/redis/sentinel3:/opt/bitnami/redis-sentinel/logs
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "127.0.0.1", "-p", "26379", "ping"]
      interval: 5s
      timeout: 2s
      retries: 5
      start_period: 5s

  # Почтовый сервер
  mailhog:
    image: mailhog/mailhog:latest
    container_name: mailhog
    hostname: mailhog
    <<: *service_defaults
    ports:
      - "${MAILHOG_HTTP_PORT:-8025}:8025"
    volumes:
      - ./logs/mailhog:/var/log/mailhog
    entrypoint: ["/bin/sh", "-lc", "exec MailHog >> /var/log/mailhog/mailhog.log 2>&1"]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8025/ >/dev/null || wget -q -T 2 -O - http://127.0.0.1:8025/ >/dev/null || exit 1"]
      interval: 5s
      timeout: 2s
      retries: 3
      start_period: 3s

  # ClickHouse cluster (2 ноды) + zookeeper
  zookeeper-1:
    image: zookeeper:3.7.1
    container_name: zookeeper-1
    hostname: zookeeper-1
    <<: *service_defaults
    environment:
      - ZOO_CLIENT_PORT=2181
      - ZOO_LOG_DIR=/var/log/clickhouse-keeper
      - ZOO_LOG_FILE=zookeeper.log
      - ZOO_LOG4J_PROP=ERROR, ROLLINGFILE
      - ZOO_MY_ID=1
    volumes:
      - zookeeper-1-data:/data
      - zookeeper-1-log-data:/datalog
      - ./logs/clickhouse/zookeeper:/var/log/clickhouse-keeper
      - ./logs/clickhouse/zookeeper:/logs
      - ./services/zookeeper:/conf:ro
    healthcheck:
      test: ["CMD-SHELL", "(echo ruok | nc -w 2 127.0.0.1 2181 | grep -q imok) || (nc -z -w 2 127.0.0.1 2181) || (bash -lc 'exec 3<>/dev/tcp/127.0.0.1/2181')"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 10s

  zookeeper-2:
    image: zookeeper:3.7.1
    container_name: zookeeper-2
    hostname: zookeeper-2
    <<: *service_defaults
    environment:
      - ZOO_CLIENT_PORT=2181
      - ZOO_LOG_DIR=/var/log/clickhouse-keeper
      - ZOO_LOG_FILE=zookeeper.log
      - ZOO_LOG4J_PROP=ERROR, ROLLINGFILE
      - ZOO_MY_ID=2
    volumes:
      - zookeeper-2-data:/data
      - zookeeper-2-log-data:/datalog
      - ./logs/clickhouse/zookeeper2:/var/log/clickhouse-keeper
      - ./logs/clickhouse/zookeeper2:/logs
      - ./services/zookeeper:/conf:ro
    healthcheck:
      test: ["CMD-SHELL", "(echo ruok | nc -w 2 127.0.0.1 2181 | grep -q imok) || (nc -z -w 2 127.0.0.1 2181) || (bash -lc 'exec 3<>/dev/tcp/127.0.0.1/2181')"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 10s

  zookeeper-3:
    image: zookeeper:3.7.1
    container_name: zookeeper-3
    hostname: zookeeper-3
    <<: *service_defaults
    environment:
      - ZOO_CLIENT_PORT=2181
      - ZOO_LOG_DIR=/var/log/clickhouse-keeper
      - ZOO_LOG_FILE=zookeeper.log
      - ZOO_LOG4J_PROP=ERROR, ROLLINGFILE
      - ZOO_MY_ID=3
    volumes:
      - zookeeper-3-data:/data
      - zookeeper-3-log-data:/datalog
      - ./logs/clickhouse/zookeeper3:/var/log/clickhouse-keeper
      - ./logs/clickhouse/zookeeper3:/logs
      - ./services/zookeeper:/conf:ro
    healthcheck:
      test: ["CMD-SHELL", "(echo ruok | nc -w 2 127.0.0.1 2181 | grep -q imok) || (nc -z -w 2 127.0.0.1 2181) || (bash -lc 'exec 3<>/dev/tcp/127.0.0.1/2181')"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 10s

  clickhouse-1:
    image: clickhouse/clickhouse-server:22.8.11.15-alpine
    container_name: clickhouse-1
    hostname: clickhouse-1
    <<: *service_defaults
    depends_on:
      zookeeper-1:
        condition: service_healthy
      zookeeper-2:
        condition: service_healthy
      zookeeper-3:
        condition: service_healthy
    environment:
      - SHARD=1
      - REPLICA=1
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    volumes:
      - clickhouse-1-data:/var/lib/clickhouse
      - ./logs/clickhouse/node1:/var/log/clickhouse-server
      - ./services/clickhouse/client/config.xml:/etc/clickhouse-client/config.d/config.xml:ro
      - ./services/clickhouse/server/config.xml:/etc/clickhouse-server/config.xml:ro
      - ./services/clickhouse/server/users.xml:/etc/clickhouse-server/users.xml:ro
      - ./services/clickhouse/server/config.d/docker_related_config.xml:/etc/clickhouse-server/config.d/docker_related_config.xml:ro
    ports:
      - "${CLICKHOUSE_HTTP_PORT:-8123}:8123"  # HTTP
      - "${CLICKHOUSE_TCP_HOST_PORT:-9004}:9000"  # TCP
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8123/ping >/dev/null || wget -q -T 2 -O - http://127.0.0.1:8123/ping >/dev/null || bash -lc 'exec 3<>/dev/tcp/127.0.0.1/8123 && printf \"GET /ping HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n\" >&3 && head -n 1 <&3 | grep -q \"200\"'"]
      interval: 5s
      timeout: 4s
      retries: 12
      start_period: 30s

  clickhouse-2:
    image: clickhouse/clickhouse-server:22.8.11.15-alpine
    container_name: clickhouse-2
    hostname: clickhouse-2
    <<: *service_defaults
    depends_on:
      zookeeper-1:
        condition: service_healthy
      zookeeper-2:
        condition: service_healthy
      zookeeper-3:
        condition: service_healthy
    environment:
      - SHARD=1
      - REPLICA=2
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    volumes:
      - clickhouse-2-data:/var/lib/clickhouse
      - ./logs/clickhouse/node2:/var/log/clickhouse-server
      - ./services/clickhouse/client/config.xml:/etc/clickhouse-client/config.d/config.xml:ro
      - ./services/clickhouse/server/config.xml:/etc/clickhouse-server/config.xml:ro
      - ./services/clickhouse/server/users.xml:/etc/clickhouse-server/users.xml:ro
      - ./services/clickhouse/server/config.d/docker_related_config.xml:/etc/clickhouse-server/config.d/docker_related_config.xml:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8123/ping >/dev/null || wget -q -T 2 -O - http://127.0.0.1:8123/ping >/dev/null || bash -lc 'exec 3<>/dev/tcp/127.0.0.1/8123 && printf \"GET /ping HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n\" >&3 && head -n 1 <&3 | grep -q \"200\"'"]
      interval: 5s
      timeout: 4s
      retries: 12
      start_period: 30s

  # RabbitMQ кластер (3 ноды) + HAProxy
  rabbitmq-1:
    image: rabbitmq:3.10.0-management
    container_name: rabbitmq-1
    hostname: rabbitmq-1
    <<: *service_defaults
    environment:
      - RABBITMQ_COOKIE=${RABBITMQ_COOKIE}
      - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER}
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD}
      - RABBITMQ_LOGS=/var/log/rabbitmq/rabbit.log
    volumes:
      - rabbitmq-1-data:/var/lib/rabbitmq
      - ./logs/rabbitmq/r1:/var/log/rabbitmq
      - ./services/rabbitmq/cluster-entrypoint.sh:/usr/local/bin/cluster-entrypoint.sh:ro
      - ./services/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
    entrypoint: ["sh","/usr/local/bin/cluster-entrypoint.sh"]
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "-q", "ping"]
      interval: 5s
      timeout: 3s
      retries: 12
      start_period: 30s

  rabbitmq-2:
    image: rabbitmq:3.10.0-management
    container_name: rabbitmq-2
    hostname: rabbitmq-2
    <<: *service_defaults
    depends_on:
      rabbitmq-1:
        condition: service_healthy
    environment:
      - RABBITMQ_COOKIE=${RABBITMQ_COOKIE}
      - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER}
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD}
      - RABBITMQ_LOGS=/var/log/rabbitmq/rabbit.log
    volumes:
      - rabbitmq-2-data:/var/lib/rabbitmq
      - ./logs/rabbitmq/r2:/var/log/rabbitmq
      - ./services/rabbitmq/cluster-entrypoint.sh:/usr/local/bin/cluster-entrypoint.sh:ro
      - ./services/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
    entrypoint: ["sh","/usr/local/bin/cluster-entrypoint.sh"]
    healthcheck:
      test: ["CMD-SHELL", "/opt/rabbitmq/sbin/rabbitmq-diagnostics -q ping || /opt/rabbitmq/sbin/rabbitmq-diagnostics -q check_port_listener 5672"]
      interval: 5s
      timeout: 3s
      retries: 12
      start_period: 50s

  rabbitmq-3:
    image: rabbitmq:3.10.0-management
    container_name: rabbitmq-3
    hostname: rabbitmq-3
    <<: *service_defaults
    depends_on:
      rabbitmq-1:
        condition: service_healthy
    environment:
      - RABBITMQ_COOKIE=${RABBITMQ_COOKIE}
      - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER}
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD}
      - RABBITMQ_LOGS=/var/log/rabbitmq/rabbit.log
    volumes:
      - rabbitmq-3-data:/var/lib/rabbitmq
      - ./logs/rabbitmq/r3:/var/log/rabbitmq
      - ./services/rabbitmq/cluster-entrypoint.sh:/usr/local/bin/cluster-entrypoint.sh:ro
      - ./services/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
    entrypoint: ["sh","/usr/local/bin/cluster-entrypoint.sh"]
    healthcheck:
      test: ["CMD-SHELL", "/opt/rabbitmq/sbin/rabbitmq-diagnostics -q ping || /opt/rabbitmq/sbin/rabbitmq-diagnostics -q check_port_listener 5672"]
      interval: 5s
      timeout: 3s
      retries: 12
      start_period: 50s

  # Haproxy для PgBouncer и RabbitMQ
  haproxy:
    image: haproxy:1.5.18-alpine
    container_name: haproxy
    hostname: haproxy
    <<: *service_defaults
    depends_on:
      rabbitmq-1:
        condition: service_healthy
      rabbitmq-2:
        condition: service_healthy
      rabbitmq-3:
        condition: service_healthy
      pgbouncer:
        condition: service_started
    ports:
      - "${HAPROXY_AMQP_PORT:-5672}:5672"   # AMQP
      - "${HAPROXY_MGMT_PORT:-15672}:15672" # HTTP mgmt via HAProxy
      - "${HAPROXY_PGBOUNCER_PORT:-6432}:6432"  # PostgreSQL via PgBouncer
    volumes:
      - ./services/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
      - ./logs/haproxy:/var/log/haproxy
    command: ["sh","-c","mkdir -p /var/log/haproxy && haproxy -f /usr/local/etc/haproxy/haproxy.cfg 2>&1 | tee -a /var/log/haproxy/haproxy.log"]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:15672/health >/dev/null || wget -q -T 2 -O - http://127.0.0.1:15672/health >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 15s

  # Elasticsearch кластер (3 ноды)
  elasticsearch-1:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    container_name: elasticsearch-1
    hostname: elasticsearch-1
    <<: *service_defaults
    environment:
      - node.name=elasticsearch-1
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=elasticsearch-1,elasticsearch-2,elasticsearch-3
      - cluster.initial_master_nodes=elasticsearch-1,elasticsearch-2,elasticsearch-3
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch-1-data:/usr/share/elasticsearch/data
      - ./logs/elasticsearch/es01:/usr/share/elasticsearch/logs
      - ./services/elasticsearch/config/log4j2.properties:/usr/share/elasticsearch/config/log4j2.properties:ro
      - ./services/elasticsearch/hunspell/ru_RU:/usr/share/elasticsearch/config/hunspell/ru_RU:ro
      - ./services/elasticsearch/hunspell/ru_RU:/etc/elasticsearch/hunspell/ru_RU:ro
    ports:
      - "${ELASTICSEARCH_HTTP_PORT:-9200}:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:9200 >/dev/null || wget -q -T 2 -O - http://localhost:9200 >/dev/null || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 12
      start_period: 50s

  elasticsearch-2:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    container_name: elasticsearch-2
    hostname: elasticsearch-2
    <<: *service_defaults
    environment:
      - node.name=elasticsearch-2
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=elasticsearch-1,elasticsearch-2,elasticsearch-3
      - cluster.initial_master_nodes=elasticsearch-1,elasticsearch-2,elasticsearch-3
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch-1-data:/usr/share/elasticsearch/data
      - ./logs/elasticsearch/es02:/usr/share/elasticsearch/logs
      - ./services/elasticsearch/config/log4j2.properties:/usr/share/elasticsearch/config/log4j2.properties:ro
      - ./services/elasticsearch/hunspell/ru_RU:/usr/share/elasticsearch/config/hunspell/ru_RU:ro
      - ./services/elasticsearch/hunspell/ru_RU:/etc/elasticsearch/hunspell/ru_RU:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:9200 >/dev/null || wget -q -T 2 -O - http://localhost:9200 >/dev/null || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 12
      start_period: 50s

  elasticsearch-3:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    container_name: elasticsearch-3
    hostname: elasticsearch-3
    <<: *service_defaults
    environment:
      - node.name=elasticsearch-3
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=elasticsearch-1,elasticsearch-2,elasticsearch-3
      - cluster.initial_master_nodes=elasticsearch-1,elasticsearch-2,elasticsearch-3
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch-3-data:/usr/share/elasticsearch/data
      - ./logs/elasticsearch/es03:/usr/share/elasticsearch/logs
      - ./services/elasticsearch/config/log4j2.properties:/usr/share/elasticsearch/config/log4j2.properties:ro
      - ./services/elasticsearch/hunspell/ru_RU:/usr/share/elasticsearch/config/hunspell/ru_RU:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:9200 >/dev/null || wget -q -T 2 -O - http://localhost:9200 >/dev/null || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 12
      start_period: 50s

  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.0
    container_name: kibana
    hostname: kibana
    <<: *service_defaults
    depends_on:
      elasticsearch-1:
        condition: service_healthy
      elasticsearch-2:
        condition: service_healthy
      elasticsearch-3:
        condition: service_healthy
    environment:
      - NODE_OPTIONS=--max-old-space-size=512
    volumes:
      - ./services/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml:ro
      - ./logs/kibana:/usr/share/kibana/logs
    ports:
      - "${KIBANA_HTTP_PORT:-5601}:5601"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:5601/api/status | grep -q '\"level\":\"available\"' || curl -fsS http://127.0.0.1:5601 >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 40s

  metricbeat:
    image: docker.elastic.co/beats/metricbeat:7.17.0
    container_name: metricbeat
    hostname: metricbeat
    <<: *service_defaults
    depends_on:
      kibana:
        condition: service_healthy
      elasticsearch-1:
        condition: service_healthy
      elasticsearch-2:
        condition: service_healthy
      elasticsearch-3:
        condition: service_healthy
    volumes:
      - ./services/metricbeat/metricbeat.yml:/usr/share/metricbeat/metricbeat.yml:ro
      - ./logs/metricbeat:/usr/share/metricbeat/logs
    command: ["-c", "/usr/share/metricbeat/metricbeat.yml"]
    healthcheck:
      test: ["CMD-SHELL", "metricbeat test output -c /usr/share/metricbeat/metricbeat.yml >/dev/null 2>&1 || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s

  # MinIO (S3 api)
  minio:
    image: minio/minio:RELEASE.2023-01-25T00-19-54Z
    container_name: minio
    hostname: minio
    <<: *service_defaults
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    entrypoint: ["/bin/sh", "/usr/local/bin/minio-entrypoint.sh"]
    volumes:
      - minio-data:/data
      - ./logs/minio:/var/log/minio
      - ./services/minio/certs:/root/.minio/certs
      - ./services/minio/entrypoint.sh:/usr/local/bin/minio-entrypoint.sh:ro
    ports:
      - "${MINIO_API_PORT:-9000}:9000"
      - "${MINIO_CONSOLE_PORT:-9001}:9001"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsSk https://127.0.0.1:9000/minio/health/ready >/dev/null || wget -q -T 2 --no-check-certificate -O - https://127.0.0.1:9000/minio/health/ready >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 6
      start_period: 10s


  # Centrifugo для сообщений в реал тайм (Веб сокет)
  centrifugo:
    image: centrifugo/centrifugo:v4.1.2
    container_name: centrifugo
    hostname: centrifugo
    <<: *service_defaults
    depends_on:
      redis-master:
        condition: service_healthy
      sentinel-1:
        condition: service_healthy
      sentinel-2:
        condition: service_healthy
      sentinel-3:
        condition: service_healthy
    entrypoint: ["/bin/sh", "/usr/local/bin/centrifugo-entrypoint.sh"]
    volumes:
      - ./services/centrifugo/config.json:/etc/centrifugo/config.json:ro
      - ./services/centrifugo/entrypoint.sh:/usr/local/bin/centrifugo-entrypoint.sh:ro
      - ./logs/centrifugo:/var/log/centrifugo
    ports:
      - "${CENTRIFUGO_HTTP_PORT:-8000}:8000"
    ulimits:
      nofile:
        soft: 65535
        hard: 65535
    healthcheck:
      test: ["CMD-SHELL", "wget -q -T 2 -O - http://localhost:8000/health >/dev/null || wget -q -T 2 -O - http://localhost:8000/ >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s

  # Logrotate для ротации логов
  logrotate:
    build:
      context: ./services/logrotate
      dockerfile: Dockerfile
    image: local/logrotate:alpine
    container_name: logrotate
    hostname: logrotate
    <<: *service_defaults
    volumes:
      - ./services/logrotate/logrotate.conf:/etc/logrotate.conf:ro
      - ./services/logrotate/conf.d:/etc/logrotate.d:ro
      - ./logs:/var/log/infra
    healthcheck:
      test: ["CMD-SHELL", "pgrep -x crond >/dev/null 2>&1 || ps | grep -q '[c]rond'"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

networks:
  infrastructure:
    driver: bridge
    name: infrastructure

volumes:
  postgres-master-data:
  postgres-replica-data:
  redis-master-data:
  redis-slave-data:
  zookeeper-1-data:
  zookeeper-2-data:
  zookeeper-3-data:
  zookeeper-1-log-data:
  zookeeper-2-log-data:
  zookeeper-3-log-data:
  clickhouse-1-data:
  clickhouse-2-data:
  rabbitmq-1-data:
  rabbitmq-2-data:
  rabbitmq-3-data:
  elasticsearch-1-data:
  elasticsearch-2-data:
  elasticsearch-3-data:
  minio-data:
