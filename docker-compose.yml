x-service-defaults: &service_defaults
  restart: unless-stopped
  networks:
    - infrastructure

services:
  # Прокси с TLS
  nginx:
    image: nginx:1.23.1-alpine
    container_name: infra_nginx
    <<: *service_defaults
    depends_on:
      php-fpm:
        condition: service_started
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./services/nginx/conf/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./services/nginx/conf/conf.d:/etc/nginx/conf.d:ro
      - ./services/nginx/certs:/etc/nginx/certs:ro
      - ./logs/nginx:/var/log/nginx
      - ./src:/var/www/html:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1/healthz >/dev/null || curl -fsS http://127.0.0.1/healthz >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s

# Веб приложение
  php-fpm:
    build:
      context: ./services/php/fpm
      dockerfile: Dockerfile
    image: local/php-fpm:alpine
    container_name: infra_php_fpm
    <<: *service_defaults
    environment:
      - PHP_MEMORY_LIMIT=512M
    volumes:
      - ./src:/var/www/html
      - ./logs/php:/var/log/php
      - ./services/php/fpm/conf/php.ini:/usr/local/etc/php/php.ini:ro
      - ./services/php/fpm/conf/www.conf:/usr/local/etc/php-fpm.d/www.conf:ro

  nodejs:
    build:
      context: ./services/nodejs
      dockerfile: Dockerfile
    image: local/node:alpine
    container_name: infra_nodejs
    <<: *service_defaults
    command: ["node", "-e", "const http=require('http');const port=parseInt(process.env.HEALTH_PORT||'3000',10);http.createServer((req,res)=>{if(req.url==='/health'){res.writeHead(200,{'Content-Type':'text/plain'});return res.end('OK');}res.statusCode=404;res.end('Not Found');}).listen(port,()=>console.log('Health server on',port));process.on('SIGTERM',()=>process.exit(0));"]
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:$${HEALTH_PORT:-3000}/health | grep -q 'OK'"]
      interval: 5s
      timeout: 2s
      retries: 5
      start_period: 3s
    volumes:
      - ./src:/usr/src/app
      - ./logs/nodejs:/var/log/node

  # PostgreSQL кластер + PGBouncer + Haproxy
  postgres-master:
    image: bitnami/postgresql-repmgr:15.10.0
    container_name: infra_pg_master
    <<: *service_defaults
    environment:
      - REPMGR_PASSWORD=${POSTGRES_REPMGR_PASSWORD}
      - POSTGRESQL_POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRESQL_USERNAME=${POSTGRES_USER}
      - POSTGRESQL_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRESQL_DATABASE=${POSTGRES_DB}
      - REPMGR_PRIMARY_ROLE_WAIT_FOR_SYNC=on
      - REPMGR_NODE_ID=1
      - REPMGR_NODE_NAME=pg-0
      - REPMGR_NODE_NETWORK_NAME=postgres-master
      - REPMGR_PRIMARY_HOST=postgres-master
      - REPMGR_PARTNER_NODES=postgres-master,postgres-replica
      - REPMGR_PORT_NUMBER=5432
      - POSTGRESQL_SHARED_PRELOAD_LIBRARIES=repmgr
      - POSTGRESQL_LOG_HOSTNAME=true
      - POSTGRESQL_LOG_CONNECTIONS=true
    volumes:
      - pg_master_data:/bitnami/postgresql
      - ./logs/postgres/master:/bitnami/postgresql/logs
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h 127.0.0.1 -p 5432 -U $${POSTGRESQL_USERNAME:-postgres} || /opt/bitnami/postgresql/bin/pg_isready -h 127.0.0.1 -p 5432 -U $${POSTGRESQL_USERNAME:-postgres}"]
      interval: 5s
      timeout: 3s
      retries: 6
      start_period: 30s

  postgres-replica:
    image: bitnami/postgresql-repmgr:15.10.0
    container_name: infra_pg_replica
    <<: *service_defaults
    depends_on:
      - postgres-master
    environment:
      - REPMGR_PASSWORD=${POSTGRES_REPMGR_PASSWORD}
      - POSTGRESQL_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRESQL_USERNAME=${POSTGRES_USER}
      - REPMGR_NODE_ID=2
      - REPMGR_NODE_NAME=pg-1
      - REPMGR_NODE_NETWORK_NAME=postgres-replica
      - REPMGR_PRIMARY_HOST=postgres-master
      - REPMGR_PARTNER_NODES=postgres-master,postgres-replica
      - REPMGR_PORT_NUMBER=5432
      - REPMGR_PRIMARY_ROLE_WAIT_FOR_SYNC=on
    volumes:
      - pg_replica_data:/bitnami/postgresql
      - ./logs/postgres/replica:/bitnami/postgresql/logs
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h 127.0.0.1 -p 5432 -U $${POSTGRESQL_USERNAME:-postgres} || /opt/bitnami/postgresql/bin/pg_isready -h 127.0.0.1 -p 5432 -U $${POSTGRESQL_USERNAME:-postgres}"]
      interval: 5s
      timeout: 3s
      retries: 8
      start_period: 40s

  pgbouncer:
    image: bitnami/pgbouncer:1.24.0
    container_name: infra_pgbouncer
    <<: *service_defaults
    environment:
      - POSTGRESQL_HOST=postgres-master
      - POSTGRESQL_PORT_NUMBER=5432
      - POSTGRESQL_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRESQL_USERNAME=${POSTGRES_USER}
      - PGBOUNCER_POOL_MODE=session
      - PGBOUNCER_MAX_CLIENT_CONN=500
      - PGBOUNCER_DEFAULT_POOL_SIZE=50
      - PGBOUNCER_LISTEN_ADDR=0.0.0.0
      - PGBOUNCER_LISTEN_PORT=6432
    volumes:
      - ./services/pgbouncer/pgbouncer.ini:/bitnami/pgbouncer/conf/pgbouncer.ini:ro
      - ./logs/pgbouncer:/bitnami/pgbouncer/logs
    depends_on:
      - postgres-master
    healthcheck:
      test: ["CMD-SHELL", "pgrep -x pgbouncer >/dev/null 2>&1 || ps | grep -q '[p]gbouncer'"]
      interval: 5s
      timeout: 2s
      retries: 3
      start_period: 5s


  # Redis master/slave + Sentinel кластер
  redis-master:
    container_name: infra_redis_1
    <<: *service_defaults
    image: bitnami/redis:6.2.6
    environment:
      - REDIS_REPLICATION_MODE=master
      - ALLOW_EMPTY_PASSWORD=yes
    ports:
      - '6379:6379'
    volumes:
      - redis_master_data:/bitnami/redis/data
      - ./logs/redis/master:/opt/bitnami/redis/logs
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "127.0.0.1", "-p", "6379", "ping"]
      interval: 5s
      timeout: 2s
      retries: 3
      start_period: 5s

  redis-slave:
    container_name: infra_redis_slave_1
    <<: *service_defaults
    image: bitnami/redis:6.2.6
    environment:
      - REDIS_REPLICATION_MODE=slave
      - REDIS_MASTER_HOST=redis-master
      - ALLOW_EMPTY_PASSWORD=yes
    ports:
      - '6379'
    depends_on:
      redis-master:
        condition: service_healthy
    volumes:
      - redis_slave_data:/bitnami/redis/data
      - ./logs/redis/slave:/bitnami/log/redis
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "127.0.0.1", "-p", "6379", "ping"]
      interval: 5s
      timeout: 2s
      retries: 5
      start_period: 12s

  redis-sentinel-1:
    container_name: infra_redis_sentinel-1
    <<: *service_defaults
    image: bitnami/redis-sentinel:6.2.6
    depends_on:
      redis-master:
        condition: service_healthy
      redis-slave:
        condition: service_healthy
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
      - REDIS_MASTER_HOST=redis-master
      - REDIS_MASTER_PORT_NUMBER=6379
    ports:
      - '26379:26379'
    volumes:
      - ./logs/redis/sentinel1:/opt/bitnami/redis-sentinel/logs
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "127.0.0.1", "-p", "26379", "ping"]
      interval: 5s
      timeout: 2s
      retries: 5
      start_period: 5s

  redis-sentinel-2:
    container_name: infra_redis_sentinel_2
    <<: *service_defaults
    image: bitnami/redis-sentinel:6.2.6
    depends_on:
      - redis-master
      - redis-slave
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
      - REDIS_MASTER_HOST=redis-master
      - REDIS_MASTER_PORT_NUMBER=6379
    ports:
      - '26380:26379'
    volumes:
      - ./logs/redis/sentinel2:/opt/bitnami/redis-sentinel/logs
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "127.0.0.1", "-p", "26379", "ping"]
      interval: 5s
      timeout: 2s
      retries: 5
      start_period: 5s

  redis-sentinel-3:
    container_name: infra_redis_sentinel_3
    <<: *service_defaults
    image: bitnami/redis-sentinel:6.2.6
    depends_on:
      redis-master:
        condition: service_healthy
      redis-slave:
        condition: service_healthy
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
      - REDIS_MASTER_HOST=redis-master
      - REDIS_MASTER_PORT_NUMBER=6379
    ports:
      - '26381:26379'
    volumes:
      - ./logs/redis/sentinel3:/opt/bitnami/redis-sentinel/logs
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "127.0.0.1", "-p", "26379", "ping"]
      interval: 5s
      timeout: 2s
      retries: 5
      start_period: 5s

# Почтовый сервер
  mailhog:
    image: mailhog/mailhog:latest
    container_name: infra_mailhog
    <<: *service_defaults
    ports:
      - "8025:8025"
    volumes:
      - ./logs/mailhog:/var/log/mailhog
    entrypoint: ["/bin/sh", "-lc", "exec MailHog >> /var/log/mailhog/mailhog.log 2>&1"]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8025/ >/dev/null || wget -q -T 2 -O - http://127.0.0.1:8025/ >/dev/null || exit 1"]
      interval: 5s
      timeout: 2s
      retries: 3
      start_period: 3s

  # ClickHouse cluster (2 ноды) + zookeeper
  clickhouse-zookeeper:
    image: zookeeper:3.7.1
    container_name: infra_clickhouse_zookeeper
    <<: *service_defaults
    environment:
      - ZOO_CLIENT_PORT=2181
      - ZOO_LOG_DIR=/var/log/clickhouse-keeper
      - ZOO_LOG_FILE=zookeeper.log
      - ZOO_LOG4J_PROP=ERROR, ROLLINGFILE
      - ZOO_MY_ID=1
    volumes:
      - ch_keeper_data:/data
      - ch_keeper_datalog:/datalog
      - ./logs/clickhouse/zookeeper:/var/log/clickhouse-keeper
      - ./logs/clickhouse/zookeeper:/logs
      - ./services/zookeeper:/conf:ro
    healthcheck:
      test: ["CMD-SHELL", "(echo ruok | nc -w 2 127.0.0.1 2181 | grep -q imok) || (nc -z -w 2 127.0.0.1 2181) || (bash -lc 'exec 3<>/dev/tcp/127.0.0.1/2181')"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 10s

  clickhouse-zookeeper-2:
    image: zookeeper:3.7.1
    container_name: infra_clickhouse_zookeeper_2
    <<: *service_defaults
    environment:
      - ZOO_CLIENT_PORT=2181
      - ZOO_LOG_DIR=/var/log/clickhouse-keeper
      - ZOO_LOG_FILE=zookeeper.log
      - ZOO_LOG4J_PROP=ERROR, ROLLINGFILE
      - ZOO_MY_ID=2
    volumes:
      - ch_keeper_data2:/data
      - ch_keeper_datalog2:/datalog
      - ./logs/clickhouse/zookeeper2:/var/log/clickhouse-keeper
      - ./logs/clickhouse/zookeeper2:/logs
      - ./services/zookeeper:/conf:ro
    healthcheck:
      test: ["CMD-SHELL", "(echo ruok | nc -w 2 127.0.0.1 2181 | grep -q imok) || (nc -z -w 2 127.0.0.1 2181) || (bash -lc 'exec 3<>/dev/tcp/127.0.0.1/2181')"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 10s

  clickhouse-zookeeper-3:
    image: zookeeper:3.7.1
    container_name: infra_clickhouse_zookeeper_3
    <<: *service_defaults
    environment:
      - ZOO_CLIENT_PORT=2181
      - ZOO_LOG_DIR=/var/log/clickhouse-keeper
      - ZOO_LOG_FILE=zookeeper.log
      - ZOO_LOG4J_PROP=ERROR, ROLLINGFILE
      - ZOO_MY_ID=3
    volumes:
      - ch_keeper_data3:/data
      - ch_keeper_datalog3:/datalog
      - ./logs/clickhouse/zookeeper3:/var/log/clickhouse-keeper
      - ./logs/clickhouse/zookeeper3:/logs
      - ./services/zookeeper:/conf:ro
    healthcheck:
      test: ["CMD-SHELL", "(echo ruok | nc -w 2 127.0.0.1 2181 | grep -q imok) || (nc -z -w 2 127.0.0.1 2181) || (bash -lc 'exec 3<>/dev/tcp/127.0.0.1/2181')"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 10s

  clickhouse-1:
    image: clickhouse/clickhouse-server:22.8.11.15-alpine
    container_name: infra_clickhouse_1
    hostname: clickhouse-1
    <<: *service_defaults
    depends_on:
      clickhouse-zookeeper:
        condition: service_healthy
      clickhouse-zookeeper-2:
        condition: service_healthy
      clickhouse-zookeeper-3:
        condition: service_healthy
    environment:
      - SHARD=1
      - REPLICA=1
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    volumes:
      - ch1_data:/var/lib/clickhouse
      - ./logs/clickhouse/node1:/var/log/clickhouse-server
      - ./services/clickhouse/client/config.xml:/etc/clickhouse-client/config.d/config.xml:ro
      - ./services/clickhouse/server/config.xml:/etc/clickhouse-server/config.xml:ro
      - ./services/clickhouse/server/users.xml:/etc/clickhouse-server/users.xml:ro
      - ./services/clickhouse/server/config.d/docker_related_config.xml:/etc/clickhouse-server/config.d/docker_related_config.xml:ro
    ports:
      - "8123:8123"  # HTTP
      - "9004:9000"  # TCP
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8123/ping >/dev/null || wget -q -T 2 -O - http://127.0.0.1:8123/ping >/dev/null || bash -lc 'exec 3<>/dev/tcp/127.0.0.1/8123 && printf \"GET /ping HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n\" >&3 && head -n 1 <&3 | grep -q \"200\"'"]
      interval: 5s
      timeout: 4s
      retries: 12
      start_period: 30s

  clickhouse-2:
    image: clickhouse/clickhouse-server:22.8.11.15-alpine
    container_name: infra_clickhouse_2
    hostname: clickhouse-2
    <<: *service_defaults
    depends_on:
      clickhouse-zookeeper:
        condition: service_healthy
      clickhouse-zookeeper-2:
        condition: service_healthy
      clickhouse-zookeeper-3:
        condition: service_healthy
    environment:
      - SHARD=1
      - REPLICA=2
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    volumes:
      - ch2_data:/var/lib/clickhouse
      - ./logs/clickhouse/node2:/var/log/clickhouse-server
      - ./services/clickhouse/client/config.xml:/etc/clickhouse-client/config.d/config.xml:ro
      - ./services/clickhouse/server/config.xml:/etc/clickhouse-server/config.xml:ro
      - ./services/clickhouse/server/users.xml:/etc/clickhouse-server/users.xml:ro
      - ./services/clickhouse/server/config.d/docker_related_config.xml:/etc/clickhouse-server/config.d/docker_related_config.xml:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8123/ping >/dev/null || wget -q -T 2 -O - http://127.0.0.1:8123/ping >/dev/null || bash -lc 'exec 3<>/dev/tcp/127.0.0.1/8123 && printf \"GET /ping HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n\" >&3 && head -n 1 <&3 | grep -q \"200\"'"]
      interval: 5s
      timeout: 4s
      retries: 12
      start_period: 30s

  # RabbitMQ кластер (3 ноды) + HAProxy
  rabbit-1:
    image: rabbitmq:3.10.0-management
    container_name: infra_rabbit_1
    <<: *service_defaults
    hostname: rabbit-1
    environment:
      - RABBITMQ_COOKIE=${RABBITMQ_COOKIE}
      - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER}
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD}
      - RABBITMQ_LOGS=/var/log/rabbitmq/rabbit.log
    volumes:
      - rabbit1_data:/var/lib/rabbitmq
      - ./logs/rabbitmq/r1:/var/log/rabbitmq
      - ./services/rabbitmq/cluster-entrypoint.sh:/usr/local/bin/cluster-entrypoint.sh:ro
      - ./services/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
    entrypoint: ["sh","/usr/local/bin/cluster-entrypoint.sh"]
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "-q", "ping"]
      interval: 5s
      timeout: 3s
      retries: 12
      start_period: 30s

  rabbit-2:
    image: rabbitmq:3.10.0-management
    container_name: infra_rabbit_2
    <<: *service_defaults
    hostname: rabbit-2
    depends_on:
      rabbit-1:
        condition: service_healthy
    environment:
      - RABBITMQ_COOKIE=${RABBITMQ_COOKIE}
      - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER}
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD}
      - RABBITMQ_LOGS=/var/log/rabbitmq/rabbit.log
    volumes:
      - rabbit2_data:/var/lib/rabbitmq
      - ./logs/rabbitmq/r2:/var/log/rabbitmq
      - ./services/rabbitmq/cluster-entrypoint.sh:/usr/local/bin/cluster-entrypoint.sh:ro
      - ./services/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
    entrypoint: ["sh","/usr/local/bin/cluster-entrypoint.sh"]
    healthcheck:
      test: ["CMD-SHELL", "/opt/rabbitmq/sbin/rabbitmq-diagnostics -q ping || /opt/rabbitmq/sbin/rabbitmq-diagnostics -q check_port_listener 5672"]
      interval: 5s
      timeout: 3s
      retries: 12
      start_period: 50s

  rabbit-3:
    image: rabbitmq:3.10.0-management
    container_name: infra_rabbit_3
    <<: *service_defaults
    hostname: rabbit-3
    depends_on:
      rabbit-1:
        condition: service_healthy
    environment:
      - RABBITMQ_COOKIE=${RABBITMQ_COOKIE}
      - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER}
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD}
      - RABBITMQ_LOGS=/var/log/rabbitmq/rabbit.log
    volumes:
      - rabbit3_data:/var/lib/rabbitmq
      - ./logs/rabbitmq/r3:/var/log/rabbitmq
      - ./services/rabbitmq/cluster-entrypoint.sh:/usr/local/bin/cluster-entrypoint.sh:ro
      - ./services/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
    entrypoint: ["sh","/usr/local/bin/cluster-entrypoint.sh"]
    healthcheck:
      test: ["CMD-SHELL", "/opt/rabbitmq/sbin/rabbitmq-diagnostics -q ping || /opt/rabbitmq/sbin/rabbitmq-diagnostics -q check_port_listener 5672"]
      interval: 5s
      timeout: 3s
      retries: 12
      start_period: 50s

# Haproxy для PgBouncer и RabbitMQ
  haproxy:
    image: haproxy:1.5.18-alpine
    container_name: infra_haproxy
    <<: *service_defaults
    depends_on:
      rabbit-1:
        condition: service_healthy
      rabbit-2:
        condition: service_healthy
      rabbit-3:
        condition: service_healthy
      pgbouncer:
        condition: service_started
    ports:
      - "5672:5672"   # AMQP
      - "15672:15672" # HTTP mgmt via HAProxy
      - "6432:6432"  # PostgreSQL via PgBouncer
    volumes:
      - ./services/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
      - ./logs/haproxy:/var/log/haproxy
    command: ["sh","-c","mkdir -p /var/log/haproxy && haproxy -f /usr/local/etc/haproxy/haproxy.cfg 2>&1 | tee -a /var/log/haproxy/haproxy.log"]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:15672/health >/dev/null || wget -q -T 2 -O - http://127.0.0.1:15672/health >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 15s

  # Elasticsearch кластер (3 ноды)
  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    container_name: infra_es01
    <<: *service_defaults
    environment:
      - node.name=es01
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es01_data:/usr/share/elasticsearch/data
      - ./logs/elasticsearch/es01:/usr/share/elasticsearch/logs
      - ./services/elasticsearch/config/log4j2.properties:/usr/share/elasticsearch/config/log4j2.properties:ro
      - ./services/elasticsearch/hunspell/ru_RU:/usr/share/elasticsearch/config/hunspell/ru_RU:ro
      - ./services/elasticsearch/hunspell/ru_RU:/etc/elasticsearch/hunspell/ru_RU:ro
    ports:
      - "9200:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:9200 >/dev/null || wget -q -T 2 -O - http://localhost:9200 >/dev/null || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 12
      start_period: 50s

  es02:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    container_name: infra_es02
    <<: *service_defaults
    environment:
      - node.name=es02
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es02_data:/usr/share/elasticsearch/data
      - ./logs/elasticsearch/es02:/usr/share/elasticsearch/logs
      - ./services/elasticsearch/config/log4j2.properties:/usr/share/elasticsearch/config/log4j2.properties:ro
      - ./services/elasticsearch/hunspell/ru_RU:/usr/share/elasticsearch/config/hunspell/ru_RU:ro
      - ./services/elasticsearch/hunspell/ru_RU:/etc/elasticsearch/hunspell/ru_RU:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:9200 >/dev/null || wget -q -T 2 -O - http://localhost:9200 >/dev/null || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 12
      start_period: 50s

  es03:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
    container_name: infra_es03
    <<: *service_defaults
    environment:
      - node.name=es03
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es03_data:/usr/share/elasticsearch/data
      - ./logs/elasticsearch/es03:/usr/share/elasticsearch/logs
      - ./services/elasticsearch/config/log4j2.properties:/usr/share/elasticsearch/config/log4j2.properties:ro
      - ./services/elasticsearch/hunspell/ru_RU:/usr/share/elasticsearch/config/hunspell/ru_RU:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:9200 >/dev/null || wget -q -T 2 -O - http://localhost:9200 >/dev/null || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 12
      start_period: 50s

  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.0
    container_name: infra_kibana
    <<: *service_defaults
    depends_on:
      es01:
        condition: service_healthy
      es02:
        condition: service_healthy
      es03:
        condition: service_healthy
    environment:
      - NODE_OPTIONS=--max-old-space-size=512
    volumes:
      - ./services/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml:ro
    ports:
      - "5601:5601"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:5601/api/status | grep -q '\"level\":\"available\"' || curl -fsS http://127.0.0.1:5601 >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 40s

  metricbeat:
    image: docker.elastic.co/beats/metricbeat:7.17.0
    container_name: infra_metricbeat
    <<: *service_defaults
    depends_on:
      kibana:
        condition: service_healthy
      es01:
        condition: service_healthy
      es02:
        condition: service_healthy
      es03:
        condition: service_healthy
    volumes:
      - ./services/metricbeat/metricbeat.yml:/usr/share/metricbeat/metricbeat.yml:ro
    command: ["-e", "-c", "/usr/share/metricbeat/metricbeat.yml"]
    healthcheck:
      test: ["CMD-SHELL", "metricbeat test output -c /usr/share/metricbeat/metricbeat.yml >/dev/null 2>&1 || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s

  # MinIO (S3 api)
  minio:
    image: minio/minio:RELEASE.2023-01-25T00-19-54Z
    container_name: infra_minio
    <<: *service_defaults
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    entrypoint: ["/bin/sh", "/usr/local/bin/minio-entrypoint.sh"]
    volumes:
      - minio_data:/data
      - ./logs/minio:/var/log/minio
      - ./services/minio/certs:/root/.minio/certs
      - ./services/minio/entrypoint.sh:/usr/local/bin/minio-entrypoint.sh:ro
    ports:
      - "9000:9000"
      - "9001:9001"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsSk https://127.0.0.1:9000/minio/health/ready >/dev/null || wget -q -T 2 --no-check-certificate -O - https://127.0.0.1:9000/minio/health/ready >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 6
      start_period: 10s


  # Centrifugo для сообщений в реал тайм (Веб сокет)
  centrifugo:
    image: centrifugo/centrifugo:v4.1.2
    container_name: infra_centrifugo
    <<: *service_defaults
    depends_on:
      redis-sentinel-1:
        condition: service_healthy
      redis-sentinel-2:
        condition: service_healthy
      redis-sentinel-3:
        condition: service_healthy
    entrypoint: ["/bin/sh", "/usr/local/bin/centrifugo-entrypoint.sh"]
    volumes:
      - ./services/centrifugo/config.json:/etc/centrifugo/config.json:ro
      - ./services/centrifugo/entrypoint.sh:/usr/local/bin/centrifugo-entrypoint.sh:ro
      - ./logs/centrifugo:/var/log/centrifugo
    ports:
      - "8000:8000"
    ulimits:
      nofile:
        soft: 65535
        hard: 65535
    healthcheck:
      test: ["CMD-SHELL", "wget -q -T 2 -O - http://localhost:8000/health >/dev/null || wget -q -T 2 -O - http://localhost:8000/ >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s

  # Logrotate для ротации логов
  logrotate:
    build:
      context: ./services/logrotate
      dockerfile: Dockerfile
    image: local/logrotate:alpine
    container_name: infra_logrotate
    <<: *service_defaults
    volumes:
      - ./services/logrotate/logrotate.conf:/etc/logrotate.conf:ro
      - ./services/logrotate/conf.d:/etc/logrotate.d:ro
      - ./logs:/var/log/infra
    healthcheck:
      test: ["CMD-SHELL", "pgrep -x crond >/dev/null 2>&1 || ps | grep -q '[c]rond'"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

networks:
  infrastructure:
    driver: bridge
    name: infrastructure

volumes:
  pg_master_data:
  pg_replica_data:
  redis_master_data:
  redis_slave_data:
  ch_keeper_data:
  ch_keeper_data2:
  ch_keeper_data3:
  ch_keeper_datalog:
  ch_keeper_datalog2:
  ch_keeper_datalog3:
  ch1_data:
  ch2_data:
  rabbit1_data:
  rabbit2_data:
  rabbit3_data:
  es01_data:
  es02_data:
  es03_data:
  minio_data:
